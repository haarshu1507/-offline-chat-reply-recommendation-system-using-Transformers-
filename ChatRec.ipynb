{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13290865,"sourceType":"datasetVersion","datasetId":8423587},{"sourceId":13290961,"sourceType":"datasetVersion","datasetId":8423659}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport nltk \nimport joblib\nimport os\nimport re\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:11:32.439108Z","iopub.execute_input":"2025-10-07T14:11:32.439363Z","iopub.status.idle":"2025-10-07T14:12:04.752804Z","shell.execute_reply.started":"2025-10-07T14:11:32.439342Z","shell.execute_reply":"2025-10-07T14:12:04.752186Z"}},"outputs":[{"name":"stderr","text":"2025-10-07 14:11:52.196460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759846312.445253      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759846312.509935      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"MODEL_NAME = 'gpt2'\n\nUSER_A_TOKEN = \"<|user_a|>\"\nUSER_B_TOKEN = \"<|user_b|>\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:04.753878Z","iopub.execute_input":"2025-10-07T14:12:04.754438Z","iopub.status.idle":"2025-10-07T14:12:04.758276Z","shell.execute_reply.started":"2025-10-07T14:12:04.754418Z","shell.execute_reply":"2025-10-07T14:12:04.757417Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"DATA_PATH = '/Desktop/Dataset/conversationfile.xlsx'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:04.758944Z","iopub.execute_input":"2025-10-07T14:12:04.759366Z","iopub.status.idle":"2025-10-07T14:12:04.780104Z","shell.execute_reply.started":"2025-10-07T14:12:04.759348Z","shell.execute_reply":"2025-10-07T14:12:04.779471Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"CONTEXT_WINDOW = 12\n\nEPOCHS = 100\nLEARNING_RATE = 5e-5\nBATCH_SIZE = 4\nMAX_LEN = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:04.781514Z","iopub.execute_input":"2025-10-07T14:12:04.781987Z","iopub.status.idle":"2025-10-07T14:12:04.795041Z","shell.execute_reply.started":"2025-10-07T14:12:04.781963Z","shell.execute_reply":"2025-10-07T14:12:04.794267Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"OUTPUT_DIR = './submission_folder/'\nMODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, 'Model.joblib')\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:04.795778Z","iopub.execute_input":"2025-10-07T14:12:04.795961Z","iopub.status.idle":"2025-10-07T14:12:04.808459Z","shell.execute_reply.started":"2025-10-07T14:12:04.795946Z","shell.execute_reply":"2025-10-07T14:12:04.807940Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_and_prepare_data(file_path):\n    try:\n        df = pd.read_excel(file_path)\n        \n        df.columns = [col.strip() for col in df.columns]\n        \n        required_cols = ['Conversation ID', 'Timestamp', 'Sender', 'Message']\n        for col in required_cols:\n            if col not in df.columns:\n                raise ValueError(f\"Missing required column: {col}\")\n        \n        # Clean data: remove rows with empty messages and strip whitespace\n        df.dropna(subset=['Message'], inplace=True)\n        df['Message'] = df['Message'].astype(str).str.strip()\n        df['Sender'] = df['Sender'].str.strip()\n        \n        # Sort values to ensure chronological order within each conversation\n        df.sort_values(by=['Conversation ID', 'Timestamp'], inplace=True)\n        \n        print(\"Data loaded and prepared successfully.\")\n        return df.reset_index(drop=True)\n        \n    except FileNotFoundError:\n        print(f\"Error: The file was not found at {file_path}\")\n        print(\"Please ensure the DATA_PATH in Section 2 is correct.\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred while loading data: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:04.809175Z","iopub.execute_input":"2025-10-07T14:12:04.809368Z","iopub.status.idle":"2025-10-07T14:12:04.822148Z","shell.execute_reply.started":"2025-10-07T14:12:04.809346Z","shell.execute_reply":"2025-10-07T14:12:04.821435Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_training_examples(df, tokenizer, context_window):\n    training_texts = []\n    for convo_id, group in df.groupby('Conversation ID'):\n        messages = group['Message'].tolist()\n        senders = group['Sender'].tolist()\n        \n        for i in range(1, len(messages)):\n            if senders[i] == 'User A' and senders[i-1] == 'User B':\n                # Context is the preceding messages in the *same conversation*.\n                start_index = max(0, i - context_window)\n                context_slice = list(zip(senders[start_index:i], messages[start_index:i]))\n                \n                prompt = \"\"\n                for sender, message in context_slice:\n                    sender_token = USER_A_TOKEN if sender == 'User A' else USER_B_TOKEN\n                    prompt += f\"{sender_token} {message} \"\n                \n                # The target reply is the current message from User A.\n                reply = f\"{USER_A_TOKEN} {messages[i]}\"\n                \n                # Combine into a single string for the language model.\n                training_texts.append(f\"{prompt.strip()} {reply.strip()} {tokenizer.eos_token}\")\n\n    print(f\"Created {len(training_texts)} training examples.\")\n    return training_texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:04.822902Z","iopub.execute_input":"2025-10-07T14:12:04.823343Z","iopub.status.idle":"2025-10-07T14:12:04.840096Z","shell.execute_reply.started":"2025-10-07T14:12:04.823320Z","shell.execute_reply":"2025-10-07T14:12:04.839480Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"--- Starting Data Processing ---\")\nconversation_df = load_and_prepare_data(DATA_PATH)\n\nif conversation_df is None:\n    raise SystemExit(\"Stopping execution due to data loading failure.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:04.840909Z","iopub.execute_input":"2025-10-07T14:12:04.841110Z","iopub.status.idle":"2025-10-07T14:12:05.355293Z","shell.execute_reply.started":"2025-10-07T14:12:04.841094Z","shell.execute_reply":"2025-10-07T14:12:05.354634Z"}},"outputs":[{"name":"stdout","text":"--- Starting Data Processing ---\nData loaded and prepared successfully.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:05.356048Z","iopub.execute_input":"2025-10-07T14:12:05.356572Z","iopub.status.idle":"2025-10-07T14:12:06.077043Z","shell.execute_reply.started":"2025-10-07T14:12:05.356537Z","shell.execute_reply":"2025-10-07T14:12:06.076180Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"special_tokens_dict = {'additional_special_tokens': [USER_A_TOKEN, USER_B_TOKEN], 'pad_token': '<|pad|>'}\ntokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))\nprint(\"Tokenizer and Model loaded and configured.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:06.079710Z","iopub.execute_input":"2025-10-07T14:12:06.079984Z","iopub.status.idle":"2025-10-07T14:12:08.457043Z","shell.execute_reply.started":"2025-10-07T14:12:06.079968Z","shell.execute_reply":"2025-10-07T14:12:08.456255Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"Tokenizer and Model loaded and configured.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"training_data = create_training_examples(conversation_df, tokenizer, CONTEXT_WINDOW)\nif not training_data:\n    raise ValueError(\"No valid training examples could be created. Please check the dataset for conversations where User A replies to User B.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:08.457825Z","iopub.execute_input":"2025-10-07T14:12:08.458026Z","iopub.status.idle":"2025-10-07T14:12:08.465172Z","shell.execute_reply.started":"2025-10-07T14:12:08.458011Z","shell.execute_reply":"2025-10-07T14:12:08.464431Z"}},"outputs":[{"name":"stdout","text":"Created 9 training examples.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"train_texts, val_texts = train_test_split(training_data, test_size=0.1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:08.466012Z","iopub.execute_input":"2025-10-07T14:12:08.466241Z","iopub.status.idle":"2025-10-07T14:12:08.517971Z","shell.execute_reply.started":"2025-10-07T14:12:08.466216Z","shell.execute_reply":"2025-10-07T14:12:08.517317Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class ChatDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.examples = []\n        for text in tqdm(texts, desc=\"Tokenizing data\"):\n            tokenized_output = self.tokenizer(\n                text,\n                truncation=True,\n                max_length=self.max_length,\n                padding=\"max_length\",\n                return_tensors=\"pt\"\n            )\n            self.examples.append(tokenized_output)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        item = self.examples[idx]\n        # For a Causal Language Model like GPT-2, labels are the same as input_ids.\n        return {\n            \"input_ids\": item['input_ids'].flatten(),\n            \"attention_mask\": item['attention_mask'].flatten(),\n            \"labels\": item['input_ids'].flatten() \n        }\n\nprint(\"\\n--- Creating Datasets and Dataloaders ---\")\ntrain_dataset = ChatDataset(train_texts, tokenizer, MAX_LEN)\nval_dataset = ChatDataset(val_texts, tokenizer, MAX_LEN)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\nprint(\"DataLoaders are ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:08.518862Z","iopub.execute_input":"2025-10-07T14:12:08.519176Z","iopub.status.idle":"2025-10-07T14:12:08.549328Z","shell.execute_reply.started":"2025-10-07T14:12:08.519154Z","shell.execute_reply":"2025-10-07T14:12:08.548731Z"}},"outputs":[{"name":"stdout","text":"\n--- Creating Datasets and Dataloaders ---\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing data: 100%|██████████| 8/8 [00:00<00:00, 761.41it/s]\nTokenizing data: 100%|██████████| 1/1 [00:00<00:00, 791.08it/s]","output_type":"stream"},{"name":"stdout","text":"DataLoaders are ready.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(f\"\\nModel moved to device: {device}\")\n\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:08.550030Z","iopub.execute_input":"2025-10-07T14:12:08.550892Z","iopub.status.idle":"2025-10-07T14:12:10.997806Z","shell.execute_reply.started":"2025-10-07T14:12:08.550868Z","shell.execute_reply":"2025-10-07T14:12:10.997069Z"}},"outputs":[{"name":"stdout","text":"\nModel moved to device: cuda\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(\"--- Starting Model Fine-Tuning ---\")\nfor epoch in range(EPOCHS):\n    print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n    model.train()\n    total_train_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n        model.zero_grad()\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(\n            input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        loss = outputs.loss\n        total_train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n    avg_train_loss = total_train_loss / len(train_loader)\n    print(f\"Average Training Loss for Epoch {epoch+1}: {avg_train_loss:.4f}\")\n\nprint(\"\\nFine-tuning complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:10.998682Z","iopub.execute_input":"2025-10-07T14:12:10.998871Z","iopub.status.idle":"2025-10-07T14:12:52.378044Z","shell.execute_reply.started":"2025-10-07T14:12:10.998852Z","shell.execute_reply":"2025-10-07T14:12:52.377259Z"}},"outputs":[{"name":"stdout","text":"--- Starting Model Fine-Tuning ---\n\n--- Epoch 1/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nTraining Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 1: 10.7743\n\n--- Epoch 2/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 2: 9.9142\n\n--- Epoch 3/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 3: 9.5998\n\n--- Epoch 4/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 4: 9.3946\n\n--- Epoch 5/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 5: 9.1977\n\n--- Epoch 6/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 6: 9.0104\n\n--- Epoch 7/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 7: 8.8758\n\n--- Epoch 8/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 8: 8.7036\n\n--- Epoch 9/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 9: 8.5469\n\n--- Epoch 10/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 10: 8.3707\n\n--- Epoch 11/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 11: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 11: 8.2020\n\n--- Epoch 12/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 12: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 12: 7.9901\n\n--- Epoch 13/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 13: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 13: 7.7227\n\n--- Epoch 14/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 14: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 14: 7.4464\n\n--- Epoch 15/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 15: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 15: 7.2032\n\n--- Epoch 16/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 16: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 16: 6.9916\n\n--- Epoch 17/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 17: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 17: 6.7734\n\n--- Epoch 18/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 18: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 18: 6.5679\n\n--- Epoch 19/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 19: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 19: 6.3773\n\n--- Epoch 20/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 20: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 20: 6.1777\n\n--- Epoch 21/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 21: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 21: 5.9898\n\n--- Epoch 22/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 22: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 22: 5.7886\n\n--- Epoch 23/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 23: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 23: 5.6153\n\n--- Epoch 24/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 24: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 24: 5.4268\n\n--- Epoch 25/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 25: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 25: 5.2637\n\n--- Epoch 26/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 26: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 26: 5.0533\n\n--- Epoch 27/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 27: 100%|██████████| 2/2 [00:00<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 27: 4.8749\n\n--- Epoch 28/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 28: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 28: 4.6960\n\n--- Epoch 29/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 29: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 29: 4.5084\n\n--- Epoch 30/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 30: 100%|██████████| 2/2 [00:00<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 30: 4.3277\n\n--- Epoch 31/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 31: 100%|██████████| 2/2 [00:00<00:00,  5.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 31: 4.1351\n\n--- Epoch 32/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 32: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 32: 3.9588\n\n--- Epoch 33/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 33: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 33: 3.7674\n\n--- Epoch 34/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 34: 100%|██████████| 2/2 [00:00<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 34: 3.5912\n\n--- Epoch 35/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 35: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 35: 3.4160\n\n--- Epoch 36/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 36: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 36: 3.2384\n\n--- Epoch 37/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 37: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 37: 3.0719\n\n--- Epoch 38/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 38: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 38: 2.8788\n\n--- Epoch 39/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 39: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 39: 2.7310\n\n--- Epoch 40/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 40: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 40: 2.5564\n\n--- Epoch 41/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 41: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 41: 2.3916\n\n--- Epoch 42/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 42: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 42: 2.2247\n\n--- Epoch 43/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 43: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 43: 2.0658\n\n--- Epoch 44/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 44: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 44: 1.9170\n\n--- Epoch 45/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 45: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 45: 1.7670\n\n--- Epoch 46/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 46: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 46: 1.6223\n\n--- Epoch 47/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 47: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 47: 1.5078\n\n--- Epoch 48/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 48: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 48: 1.3748\n\n--- Epoch 49/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 49: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 49: 1.2543\n\n--- Epoch 50/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 50: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 50: 1.1463\n\n--- Epoch 51/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 51: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 51: 1.0521\n\n--- Epoch 52/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 52: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 52: 0.9767\n\n--- Epoch 53/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 53: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 53: 0.8782\n\n--- Epoch 54/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 54: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 54: 0.8112\n\n--- Epoch 55/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 55: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 55: 0.7351\n\n--- Epoch 56/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 56: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 56: 0.6805\n\n--- Epoch 57/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 57: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 57: 0.6245\n\n--- Epoch 58/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 58: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 58: 0.5752\n\n--- Epoch 59/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 59: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 59: 0.5395\n\n--- Epoch 60/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 60: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 60: 0.4976\n\n--- Epoch 61/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 61: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 61: 0.4644\n\n--- Epoch 62/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 62: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 62: 0.4323\n\n--- Epoch 63/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 63: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 63: 0.4099\n\n--- Epoch 64/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 64: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 64: 0.3861\n\n--- Epoch 65/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 65: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 65: 0.3720\n\n--- Epoch 66/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 66: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 66: 0.3504\n\n--- Epoch 67/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 67: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 67: 0.3373\n\n--- Epoch 68/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 68: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 68: 0.3224\n\n--- Epoch 69/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 69: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 69: 0.3115\n\n--- Epoch 70/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 70: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 70: 0.3027\n\n--- Epoch 71/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 71: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 71: 0.2870\n\n--- Epoch 72/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 72: 100%|██████████| 2/2 [00:00<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 72: 0.2750\n\n--- Epoch 73/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 73: 100%|██████████| 2/2 [00:00<00:00,  4.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 73: 0.2719\n\n--- Epoch 74/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 74: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 74: 0.2747\n\n--- Epoch 75/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 75: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 75: 0.2558\n\n--- Epoch 76/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 76: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 76: 0.2530\n\n--- Epoch 77/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 77: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 77: 0.2516\n\n--- Epoch 78/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 78: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 78: 0.2408\n\n--- Epoch 79/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 79: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 79: 0.2415\n\n--- Epoch 80/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 80: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 80: 0.2335\n\n--- Epoch 81/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 81: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 81: 0.2313\n\n--- Epoch 82/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 82: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 82: 0.2240\n\n--- Epoch 83/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 83: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 83: 0.2234\n\n--- Epoch 84/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 84: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 84: 0.2191\n\n--- Epoch 85/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 85: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 85: 0.2191\n\n--- Epoch 86/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 86: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 86: 0.2119\n\n--- Epoch 87/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 87: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 87: 0.2131\n\n--- Epoch 88/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 88: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 88: 0.2128\n\n--- Epoch 89/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 89: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 89: 0.2030\n\n--- Epoch 90/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 90: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 90: 0.2074\n\n--- Epoch 91/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 91: 100%|██████████| 2/2 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 91: 0.2060\n\n--- Epoch 92/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 92: 100%|██████████| 2/2 [00:00<00:00,  4.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 92: 0.2050\n\n--- Epoch 93/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 93: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 93: 0.2019\n\n--- Epoch 94/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 94: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 94: 0.2027\n\n--- Epoch 95/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 95: 100%|██████████| 2/2 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 95: 0.1966\n\n--- Epoch 96/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 96: 100%|██████████| 2/2 [00:00<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 96: 0.1991\n\n--- Epoch 97/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 97: 100%|██████████| 2/2 [00:00<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 97: 0.2013\n\n--- Epoch 98/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 98: 100%|██████████| 2/2 [00:00<00:00,  5.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 98: 0.1999\n\n--- Epoch 99/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 99: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 99: 0.1945\n\n--- Epoch 100/100 ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 100: 100%|██████████| 2/2 [00:00<00:00,  4.95it/s]","output_type":"stream"},{"name":"stdout","text":"Average Training Loss for Epoch 100: 0.1992\n\nFine-tuning complete.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model_and_tokenizer_to_save = {\n    'model_state_dict': model.to('cpu').state_dict(),\n    'tokenizer': tokenizer,\n    'model_name': MODEL_NAME\n}\n\njoblib.dump(model_and_tokenizer_to_save, MODEL_SAVE_PATH)\nprint(f\"Model and tokenizer saved to {MODEL_SAVE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:52.378763Z","iopub.execute_input":"2025-10-07T14:12:52.379055Z","iopub.status.idle":"2025-10-07T14:12:55.216227Z","shell.execute_reply.started":"2025-10-07T14:12:52.379030Z","shell.execute_reply":"2025-10-07T14:12:55.215558Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved to ./submission_folder/Model.joblib\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def load_model_from_joblib(path):\n    saved_data = joblib.load(path)\n    \n    tokenizer = saved_data['tokenizer']\n    model = GPT2LMHeadModel.from_pretrained(saved_data['model_name'])\n    model.resize_token_embeddings(len(tokenizer))\n    model.load_state_dict(saved_data['model_state_dict'])\n    \n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:55.216956Z","iopub.execute_input":"2025-10-07T14:12:55.217180Z","iopub.status.idle":"2025-10-07T14:12:55.221243Z","shell.execute_reply.started":"2025-10-07T14:12:55.217162Z","shell.execute_reply":"2025-10-07T14:12:55.220446Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def generate_reply(model, tokenizer, history_df, new_message_from_b, device):\n    model.to(device)\n    model.eval()\n\n    prompt = \"\"\n    for _, row in history_df.iterrows():\n        sender_token = USER_A_TOKEN if row['Sender'] == 'User A' else USER_B_TOKEN\n        prompt += f\"{sender_token} {row['Message']} \"\n    \n    prompt += f\"{USER_B_TOKEN} {new_message_from_b} {USER_A_TOKEN}\"\n\n    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,  # Limit the length of the new reply\n            pad_token_id=tokenizer.eos_token_id,\n            no_repeat_ngram_size=2,\n            do_sample=True,     # Activate sampling\n            top_k=50,           # Consider the top 50 tokens\n            top_p=0.95,         # Use nucleus sampling\n        )\n    \n    generated_tokens = outputs[0][inputs['input_ids'].shape[-1]:]\n    reply = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n    \n    return reply","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:12:55.222062Z","iopub.execute_input":"2025-10-07T14:12:55.222314Z","iopub.status.idle":"2025-10-07T14:12:55.238275Z","shell.execute_reply.started":"2025-10-07T14:12:55.222292Z","shell.execute_reply":"2025-10-07T14:12:55.237615Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print(\"\\n--- Testing Reply Generation ---\")\n# Load the model back for a clean test\nloaded_model, loaded_tokenizer = load_model_from_joblib(MODEL_SAVE_PATH)\n\n# [cite_start]Context from Conversation ID 4 [cite: 1]\nconversation_history = pd.DataFrame([\n    {'Sender': 'User A', 'Message': \"Finally watched that new sci-fi movie everyone's talking about.\"},\n    {'Sender': 'User B', 'Message': \"Nice! What did you think? I loved the visuals.\"},\n    {'Sender': 'User A', 'Message': \"Visuals were amazing, but the plot was a bit predictable for me.\"}\n])\nuser_b_new_message = \"I can see that. The ending felt a bit rushed. Still a fun watch though.\"\n\ngenerated_reply = generate_reply(loaded_model, loaded_tokenizer, conversation_history, user_b_new_message, device)\nprint(f\"Conversation Context:\\n{conversation_history.to_string(index=False)}\\n\")\nprint(f\"User B says: '{user_b_new_message}'\")\nprint(f\"Generated User A reply: '{generated_reply}'\")\nprint(f\"Actual User A reply from data: 'Definitely. Worth it just for the big screen experience.'\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:13:14.972325Z","iopub.execute_input":"2025-10-07T14:13:14.972937Z","iopub.status.idle":"2025-10-07T14:13:17.171232Z","shell.execute_reply.started":"2025-10-07T14:13:14.972911Z","shell.execute_reply":"2025-10-07T14:13:17.170552Z"}},"outputs":[{"name":"stdout","text":"\n--- Testing Reply Generation ---\nConversation Context:\nSender                                                          Message\nUser A  Finally watched that new sci-fi movie everyone's talking about.\nUser B                   Nice! What did you think? I loved the visuals.\nUser A Visuals were amazing, but the plot was a bit predictable for me.\n\nUser B says: 'I can see that. The ending felt a bit rushed. Still a fun watch though.'\nGenerated User A reply: '\"Definitely. Worth it just for the big screen experience.\"'\nActual User A reply from data: 'Definitely. Worth it just for the big screen experience.'\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def evaluate_model(model, val_loader, tokenizer, device):\n    model.to(device)\n    model.eval()\n    \n    total_perplexity_loss = 0\n    bleu_scores = []\n    chencherry = SmoothingFunction()\n    \n    print(\"\\n--- Starting Final Evaluation ---\")\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Calculating Perplexity\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            total_perplexity_loss += outputs.loss.item()\n            \n        avg_loss = total_perplexity_loss / len(val_loader)\n        perplexity = torch.exp(torch.tensor(avg_loss))\n        print(f\"  - Perplexity on Validation Set: {perplexity.item():.4f}\")\n\n        for text in tqdm(val_texts, desc=\"Calculating BLEU Score\"):\n            prompt_end_index = text.rfind(USER_A_TOKEN)\n            if prompt_end_index == -1: continue\n\n            prompt = text[:prompt_end_index] + USER_A_TOKEN\n            reference_reply = text[prompt_end_index:].replace(USER_A_TOKEN, \"\").replace(tokenizer.eos_token, \"\").strip()\n            \n            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n            \n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=50,\n                pad_token_id=tokenizer.eos_token_id,\n                do_sample=True,\n                top_k=50,\n                top_p=0.95,\n            )\n            \n            generated_tokens = outputs[0][inputs['input_ids'].shape[-1]:]\n            generated_reply = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n\n            reference_tokens = [reference_reply.split()]\n            candidate_tokens = generated_reply.split()\n            \n            if candidate_tokens:\n                bleu_score = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=chencherry.method1)\n                bleu_scores.append(bleu_score)\n            \n    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0\n    print(f\"  - Average BLEU Score on Validation Set: {avg_bleu:.4f}\")\n    \n    return perplexity.item(), avg_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:13:20.179738Z","iopub.execute_input":"2025-10-07T14:13:20.180262Z","iopub.status.idle":"2025-10-07T14:13:20.188554Z","shell.execute_reply.started":"2025-10-07T14:13:20.180235Z","shell.execute_reply":"2025-10-07T14:13:20.187673Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"final_perplexity, final_bleu = evaluate_model(loaded_model, val_loader, loaded_tokenizer, device)\n\nprint(\"\\n--- Script Finished Successfully ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T14:13:23.235656Z","iopub.execute_input":"2025-10-07T14:13:23.235920Z","iopub.status.idle":"2025-10-07T14:13:23.656375Z","shell.execute_reply.started":"2025-10-07T14:13:23.235902Z","shell.execute_reply":"2025-10-07T14:13:23.655663Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Final Evaluation ---\n","output_type":"stream"},{"name":"stderr","text":"Calculating Perplexity: 100%|██████████| 1/1 [00:00<00:00, 58.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"  - Perplexity on Validation Set: 1.1501\n","output_type":"stream"},{"name":"stderr","text":"Calculating BLEU Score: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"  - Average BLEU Score on Validation Set: 0.4098\n\n--- Script Finished Successfully ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}